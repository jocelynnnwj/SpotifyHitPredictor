# -*- coding: utf-8 -*-
"""Final-proj-"team#2"-code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tAWatSEhZ4Z2XD27G6Owsx2HLNrfi2HC
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

"""## Data cleaning and pre-processing"""

data = pd.read_csv("./spotify_songs.csv", encoding='latin-1')
data.tail(5)

data.isna().sum()

# drop the irrelevant columns
data.drop(['track_id', 'track_name', 'track_artist', 'track_album_id',
           'track_album_name', "track_album_release_date", "playlist_name", "playlist_id",
           "playlist_genre", "playlist_subgenre"], inplace=True, axis=1)

# Display the current updated information for the dataset.
data.info()

"""# Exploratory Data Analysis"""

data.describe()

data.hist(bins=30, figsize=(15, 10))

# Standardize the columns
from sklearn import preprocessing

scaler = preprocessing.StandardScaler()
scaled_data = scaler.fit_transform(data)

# Convert the scaled data back into a DataFrame
scaled_df = pd.DataFrame(scaled_data, columns=data.columns)

scaled_df.hist(bins=30, figsize=(15, 10))

print("Mean after scaling:", data.mean(axis=0))
print("Standard deviation after scaling:", data.std(axis=0))

print("Mean after scaling:", scaled_df.mean(axis=0))
print("Standard deviation after scaling:", scaled_df.std(axis=0))

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

# plotting a correlation matrix
corr = scaled_df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f")
plt.show()

"""# Feature Selection

Now we start to build the prediction model for the `track_popularity`. First, we split the data and apply feature selection.
"""

from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import train_test_split

X = scaled_df.drop(['track_popularity'], axis = 1)
y = scaled_df['track_popularity']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Feature Selection using Forward Selection
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction='forward', scoring='r2', cv=5)
# Fit the SFS to the training data
sfs.fit(X_train, y_train)

# Get the boolean mask of the selected features
selected_features_mask = sfs.get_support()

# Get the names of the selected features from the mask
selected_features = X_train.columns[selected_features_mask]
print("Selected Features:", selected_features)

from sklearn.feature_selection import SelectKBest, f_regression

select_k_best = SelectKBest(f_regression, k=10)

select_k_best.fit(X_train, y_train)

X_train_selected = select_k_best.transform(X_train)
X_test_selected = select_k_best.transform(X_test)

# Selected features can also be identified using get_support
selected_features_skb = X_train.columns[select_k_best.get_support()]
print("Selected features using SelectKBest:", selected_features_skb)

scores = select_k_best.scores_  # Get the scores assigned to each feature
feature_scores = pd.DataFrame({'Feature': X_train.columns, 'Score': scores})

# Sort by score in descending order
feature_scores_sorted = feature_scores.sort_values(by='Score', ascending=False)
print(feature_scores_sorted)



"""# Prediction Modeling

After selected the relevant features, we move to the first objective which is to do the prediction modeling.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm

top_features = ['instrumentalness', 'duration_ms', 'energy', 'acousticness', 'danceability', 'loudness', 'liveness'] # select scores larger than 50

model = LinearRegression()
model.fit(X_train[top_features], y_train)
y_pred = model.predict(X_test[top_features])

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("R^2 Score:", r2)

# Random forest
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42) # Might need to fine tune

rf_model.fit(X_train[top_features], y_train)

y_pred = rf_model.predict(X_test[top_features])

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("R^2 Score:", r2)

# Decision Tree model
from sklearn.tree import DecisionTreeRegressor

dt_model = DecisionTreeRegressor(random_state=42)

dt_model.fit(X_train[top_features], y_train)

y_pred = dt_model.predict(X_test[top_features])

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("R^2 Score:", r2)

"""## Classification Modeling

Since the result above is not promising because it would be hard to predict the exact popularity. We simplify it as a categorization problem
"""

# Convert and create a new variable to categorize whether a song is popular
scaled_df['popularity_category'] = (scaled_df['track_popularity'] > 1).astype(int)
scaled_df

X = scaled_df.drop(['popularity_category'], axis = 1)
y = scaled_df['popularity_category']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

top_features = ['instrumentalness', 'duration_ms', 'energy', 'acousticness', 'danceability', 'loudness', 'liveness']

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

classifier = RandomForestClassifier(random_state=42)

classifier.fit(X_train[top_features], y_train)

y_pred = classifier.predict(X_test[top_features])

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC Score:", roc_auc)

# Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

logreg = LogisticRegression(random_state=42, max_iter=1000)
logreg.fit(X_train[top_features], y_train)

# Predict on the test set
y_pred_logreg = logreg.predict(X_test[top_features])

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("Precision:", precision_score(y_test, y_pred_logreg))
print("Recall:", recall_score(y_test, y_pred_logreg))
print("F1 Score:", f1_score(y_test, y_pred_logreg))

# Naive Bayes
from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train[top_features], y_train)

y_pred_nb = nb.predict(X_test[top_features])

print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print("Precision:", precision_score(y_test, y_pred_nb))
print("Recall:", recall_score(y_test, y_pred_nb))
print("F1 Score:", f1_score(y_test, y_pred_nb))

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train[top_features], y_train)

y_pred_dt = dt.predict(X_test[top_features])

print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Precision:", precision_score(y_test, y_pred_dt))
print("Recall:", recall_score(y_test, y_pred_dt))
print("F1 Score:", f1_score(y_test, y_pred_dt))